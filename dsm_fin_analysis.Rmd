---
title: DSM analysis of fin whale sightings
author: David L Miller and Doug Sigourney

---

# Introduction

These data consist of observations of fin whales as part of NOAA's [Atlantic Marine Assessment Program for Protected Species](https://www.fisheries.noaa.gov/new-england-mid-atlantic/population-assessments/atlantic-marine-assessment-program-protected).

The analysis here is based on that in *Developing and assessing a density surface model in a Bayesian hierarchical framework with a focus on uncertainty: insights from simulations and an application to fin whales (Balaenoptera physalus)* [available at PeerJ](https://peerj.com/articles/8226/).

# Elements of this analysis

- 2 detection functions
  - Aerial survey: multiple covariate distance sampling (MCDS)
  - Shipboard survey: double observer survey (mark-recapture distance sampling; MRDS)
- Availability correction for aerial surveys (0.37)
- Density surface model combining these two sources
- Comparison of using average group size vs. observed group size

Note that in the original analysis of these data a fully Bayesian model was used, incorporating uncertainty in group size and availability. Here we do not address these sources of uncertainty. The most appropriate comparison is to results provided in their [Table 3](https://peerj.com/articles/8226/#table-3) where availability was treated as constant (since uncertainty in mean group size contributed a negligible amount to uncertainty). This estimate is 4345 fin whales with a CV=0.21.


# Preliminaries

Data has already been processed and is stored in `findata.RData`.

```{r data-and-packages}
library(ggplot2)
library(patchwork)
library(mrds)
library(dsm)
load("findata.RData")
```

Note that `dsm` package version 2.3.1.9007 (development version from [github](https://github.com/distancedevelopment/dsm)) is required for multiple detection function functionality.

## Model fitting

We now fit the detection functions, followed by the density surface model. Note that we'd normally do full model selection at each stage, but since we are simply duplicating the analysis from the paper we just use the models selected there.

Note that the data used to fit the detection functions (`ship_detections` and `plane_detections`) includes detections of fin whales, sei whales and detections that could only be classified to "fin or sei whale". This improves the fit of the detection functions as more detections are included. For fitting the density surface model, we only want to include detections of fin whales (since that is the species we want an abundance estimate for). We can exclude the non-fin whale observations by including only the `object` IDs that we want in the observation frame `fin_obs`. See the data setup script for how this was done.


We first setup the truncations for each detection function model:

```{r truncation}
w_ship <- 6
w_plane <- 0.9
```

and then need to set up a vector of availabilities for the DSM. This just needs to be the same length as the observation data.

```{r availability}
a_plane <- 0.37
a_ship <- 1

avail <- c(a_ship, a_plane)[fin_obs$survey]
```



### Ship surveys - MRDS

We can fit an independent observer model to the double observer data for the ship.

```{r ship-mrds}
Ship_mrds <- ddf(dsmodel = ~mcds(key = "hr", formula = ~beaufort + SUBJ_WAVG),
                 mrmodel = ~glm(~distance),
                 data = ship_detections, method = "io",
                 meta.data = list(width = w_ship))
summary(Ship_mrds)
```


### Aerial surveys - MCDS

The aerial survey is a regular multiple covariate detection function:

```{r plane-mcds}
Plane_ds <- ddf(dsmodel = ~mcds(key = "hr", formula = ~beaufort),
                data = plane_detections, meta.data = list(width = w_plane))
summary(Plane_ds)
```


### Density surface model


In Sigourney et al. (2020) group sizes were to be 1 for fitting. Predictions were then inflated by multiplying by the average group size (1.38), as is often done with large whales with small groups sizes with little variation. We specify this by setting `group=TRUE`.

We can then fit the model using the below code. Note a list of detection function objects is provided to `ddf.obj`, their ordering matters and corresponds to the `ddfobj` column in the segment data (`fin_segs`).

```{r dsm-avg-group}
dsm_avg_group <- dsm(count ~ s(DIST125, bs="ts", k=5) +
                             s(DEPTH, bs="ts", k=5) +
                             s(DIST2SHORE, bs="ts", k=5) +
                             s(SST, bs="ts", k=5),
                     ddf.obj=list(Ship_mrds, Plane_ds),
                     family=tw(link="log"), method="REML",
                     segment.data=fin_segs, group=TRUE,
                     availability=avail,
                     observation.data=fin_obs)
```

We can look at the summary for the model and compare observed and expected number of groups by Beaufort chunk:

```{r dsm-avg-group-checking}
summary(dsm_avg_group)
obs_exp(dsm_avg_group, "beaufort", 0:5)
```

We can propagate the variance from the detection functions through to predictions over the study area from the DSM:

```{r dsm-avg-group-var}
vp_b <- dsm_varprop(dsm_avg_group, predgrid)
vp_b
```

We need to correct the estimate for group size. We can use the `predict` function to get the estimate (rather than using `dsm_varprop`):

```{r dsm-avg-group-predict}
predN <- predict(dsm_avg_group, newdata=predgrid,
                       off.set=predgrid$off.set)

# abundance estimate
sum(predN*1.38)
```


We can now fit same model but using observed rather than estimated group sizes

```{r dsm-obs-group}
dsm_obs_group <- dsm(count ~ s(DIST125, bs="ts", k=5) +
                             s(DEPTH, bs="ts", k=5) +
                             s(DIST2SHORE, bs="ts", k=5) +
                             s(SST, bs="ts", k=5),
                     ddf.obj=list(Ship_mrds, Plane_ds),
                     family=tw(link="log"), method="REML",
                     availability=avail,
                     segment.data=fin_segs,
                     observation.data=fin_obs)
```

The only difference between this model and `dsm_avg_group` is now we don't set `group=TRUE`. We can see that we get pretty similar results:

```{r dsm-obs-group-checking}
summary(dsm_obs_group)
obs_exp(dsm_obs_group, "beaufort", 0:5)
```

But we now don't need to multiply by the average group size:

```{r dsm-obs-group-var}
vp <- dsm_varprop(dsm_obs_group, predgrid)
vp
```


Preferable to use a Metropolis-Hastings sampler to get a posterior for the model. Using code from [dill/GAMsampling](https://github.com/dill/GAMsampling):

```{r mh-sampler}
# extra code
source("gam.mh_fix.r")
source("ttools.R")
source("likelihood_tools.R")

# do the sampling
# minor fiddling with the parameters here necessary
bs <- gam.mh(vp$refit, burn=2000, t.df=25, rw.scale=0.2, thin=10)
```

Now we need to construct a prediction matrix:

```{r predmat}
# extra columns for the varprop bit of the model
predgrid$XX <- matrix(0, nrow=nrow(predgrid),
                      ncol=length(coef(vp$refit))-length(coef(dsm_obs_group)))
# create the matrix that maps model coefficients to the linear predictor
Xp <- predict(vp$refit, predgrid, type="lpmatrix")
# generate predictions
preds_mh <- predgrid$off.set*exp(Xp%*%t(bs$bs))
```

Now calculate our posterior statistics per grid cell:

```{r post-stats}
# was results as density in animals/km^2
preds_mh <- preds_mh/10^2
predgrid$Density <- rowMeans(preds_mh)
predgrid$CV <- apply(preds_mh, 1, sd)/rowMeans(preds_mh)
```

Finally, plotting these:

```{r plotit, fig.width=12}
this_theme <- theme(legend.position="bottom",
                    axis.title.y=element_blank(),
                    axis.text.y=element_blank(),
                    axis.title.x=element_blank(),
                    axis.text.x=element_blank(),
                    legend.text=element_text(size=8))


# get scale for predictions as in paper
mind <- 0.000017
maxd <- 0.048
# geometrical breaks
n <- 10
k <- (maxd/mind)^(1/n)
brks <- c(0, mind, (mind*(k^seq(n))), max(predgrid$Density))
predgrid$pred_d <- cut(predgrid$Density, brks)
# nicer break labels (fiddly!)
levels(predgrid$pred_d) <- sub(",", " - ", levels(predgrid$pred_d))
levels(predgrid$pred_d) <- sub("\\(", "", levels(predgrid$pred_d))
levels(predgrid$pred_d) <- sub("]", "", levels(predgrid$pred_d))
levels(predgrid$pred_d)[1] <- "<0.000017"
levels(predgrid$pred_d)[length(levels(predgrid$pred_d))] <- ">0.048"
levels(predgrid$pred_d) <- unlist(lapply(levels(predgrid$pred_d), function(x){
  if(grepl("e", x)){
    x <- as.character(as.numeric(strsplit(x, " - ")[[1]]))
    x <- paste0(x[1], " - ", x[2])
  }
  x
}))

# and the breaks for the CV (as in paper)
cvbrks <- c(0, 0.35, .5, 1, ceiling(max(predgrid$CV)))
predgrid$CV_d <- cut(predgrid$CV, cvbrks)
# nicer break labels
levels(predgrid$CV_d) <- sub(",", " - ", levels(predgrid$CV_d))
levels(predgrid$CV_d) <- sub("\\(", "", levels(predgrid$CV_d))
levels(predgrid$CV_d) <- sub("]", "", levels(predgrid$CV_d))
levels(predgrid$CV_d)[4] <- ">1"

# land outline
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)
na <- ne_countries(continent="North America", returnclass="sf", scale=50)
na <- st_transform(na, "+proj=omerc +lat_0=35 +lonc=-75 +alpha=40 +k=0.9996 +x_0=0 +y_0=0 +gamma=40 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0") 

predplot <- ggplot(predgrid) +
  geom_tile(aes(x=x, y=y, fill=pred_d, colour=pred_d, width=10000, height=10000)) +
  geom_sf(data=na, colour="grey80", fill="grey80")+
  theme_minimal() +
  scale_fill_viridis_d() +
  scale_colour_viridis_d(guide=FALSE) +
  labs(fill="Density") +
  this_theme +
  coord_sf(xlim=range(predgrid$x), ylim=range(predgrid$y), expand=FALSE)

CVplot <- ggplot(predgrid) +
  geom_tile(aes(x=x, y=y, fill=CV_d, colour=CV_d, width=10000, height=10000)) +
  geom_sf(data=na, colour="grey80", fill="grey80")+
  theme_minimal() +
  scale_fill_viridis_d() +
  scale_colour_viridis_d(guide=FALSE) +
  labs(fill="CV") +
  this_theme +
  coord_sf(xlim=range(predgrid$x), ylim=range(predgrid$y), expand=FALSE)

# print the plots side-by-side
predplot + CVplot
```

We can also calculate an overall summary for comparison:

```{r nhat-comp}
# abundance estimate
sum(vp$pred)
# standard error
sqrt(var(colSums(preds_mh)))
# coefficient of variation
sqrt(var(colSums(preds_mh)))/(sum(vp$pred)/10^2)
```

Although we don't have an exact comparison from Sigourney et al. (2020), the authors make estimates fixing the group size to observed values ($\hat{N}$=4013; CV=0.31) and fixing the availability to its mean as we do here ($\hat{N}$=4345; CV=0.21) while leaving the rest of the model as-is.
